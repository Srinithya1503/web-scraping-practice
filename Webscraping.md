Web scraping has become the famous data collection method which can automatically extract the unstructured formatted data from websites and convert it into structured formats like excel, csv, text etc.,
HTML, CSS, and JavaScript are the foundation of the web because they provide the perfect balance of structure, style, and interactivity. Languages like Python, PHP, Java run on server side which would be employed to do scraping.

It involves the following four primary steps: 
  Send an HTTP request to a webpage 
  Receive and parse the HTML response using a library
  Extract the required data 
  Store the extracted data in a structured format 

Python’s easy-to-read syntax makes web scraping quicker and more efficient compared to other languages like Java or C++. It can handle the dynamically loading websites. Python can be integrated with:
  Databases → SQL, MongoDB
  Data visualization → Matplotlib, Seaborn
  Machine learning models for text analysis

Python libraries for webscraping: 
 requests → Sends HTTP requests
 BeautifulSoup → Parses HTML & extracts data
 Selenium → Automates browser interactions (for JavaScript-heavy websites)
 Scrapy → A powerful web scraping framework
 Playwright → Faster alternative for headless browsing 
